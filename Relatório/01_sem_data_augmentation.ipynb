{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2776efbf-9015-4f0e-8bf0-111ce43e0108",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import tensorflow as tf\n",
                "import matplotlib.pyplot as plt\n",
                "import os\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D, BatchNormalization\n",
                "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, ReduceLROnPlateau\n",
                "from tensorflow.keras.optimizers import Adam\n",
                "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2e706d27-0688-4988-9a6f-2666e784735c",
            "metadata": {},
            "outputs": [],
            "source": [
                "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
                "# CONSTANTES\n",
                "BATCH_SIZE = 32\n",
                "IMG_SIZE = 32\n",
                "NUM_CLASSES = 10  # nº classes para identificar\n",
                "NUM_EPOCHS = 60\n",
                "LEARNING_RATE = 0.001"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ec2b77b4-9d5f-4ed6-8f1f-38ab7d9f91d5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define directories\n",
                "train_dirs = ['./dataset/train/train1', './dataset/train/train2', './dataset/train/train3', './dataset/train/train5']\n",
                "validation_dir = './dataset/validation'\n",
                "test_dir = './dataset/test'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1ce2b0fa-d958-482c-a087-d53f061c54bc",
            "metadata": {},
            "outputs": [],
            "source": [
                "# CRIAR OS GERADORES\n",
                "train_datagen = ImageDataGenerator(rescale=1./255)\n",
                "\n",
                "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
                "test_datagen = ImageDataGenerator(rescale=1./255)\n",
                "\n",
                "# training generators\n",
                "train_generators = [train_datagen.flow_from_directory(\n",
                "    train_dir,\n",
                "    target_size=(IMG_SIZE, IMG_SIZE),\n",
                "    batch_size=BATCH_SIZE,\n",
                "    class_mode='categorical') for train_dir in train_dirs]\n",
                "\n",
                "# Necessário para junstar os trainning generators\n",
                "def combined_generator(generators):\n",
                "    while True:\n",
                "        for generator in generators:\n",
                "            yield next(generator)\n",
                "\n",
                "train_generator = combined_generator(train_generators)\n",
                "\n",
                "# Validation e test generators\n",
                "validation_generator = validation_datagen.flow_from_directory(\n",
                "    validation_dir,\n",
                "    target_size=(IMG_SIZE, IMG_SIZE),\n",
                "    batch_size=BATCH_SIZE,\n",
                "    class_mode='categorical')\n",
                "\n",
                "test_generator = test_datagen.flow_from_directory(\n",
                "    test_dir,\n",
                "    target_size=(IMG_SIZE, IMG_SIZE),\n",
                "    batch_size=BATCH_SIZE,\n",
                "    class_mode='categorical')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "598663f6",
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras import backend as K\n",
                "from tensorflow.keras.metrics import Metric\n",
                "\n",
                "class Precision(Metric):\n",
                "    def __init__(self, name='precision', **kwargs):\n",
                "        super(Precision, self).__init__(name=name, **kwargs)\n",
                "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
                "        self.predicted_positives = self.add_weight(name='pp', initializer='zeros')\n",
                "\n",
                "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
                "        y_pred = K.round(y_pred)\n",
                "        y_true = K.cast(y_true, 'float32')\n",
                "        self.true_positives.assign_add(K.sum(y_true * y_pred))\n",
                "        self.predicted_positives.assign_add(K.sum(y_pred))\n",
                "\n",
                "    def result(self):\n",
                "        return self.true_positives / (self.predicted_positives + K.epsilon())\n",
                "\n",
                "    def reset_states(self):\n",
                "        self.true_positives.assign(0)\n",
                "        self.predicted_positives.assign(0)\n",
                "\n",
                "class Recall(Metric):\n",
                "    def __init__(self, name='recall', **kwargs):\n",
                "        super(Recall, self).__init__(name=name, **kwargs)\n",
                "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
                "        self.actual_positives = self.add_weight(name='ap', initializer='zeros')\n",
                "\n",
                "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
                "        y_pred = K.round(y_pred)\n",
                "        y_true = K.cast(y_true, 'float32')\n",
                "        self.true_positives.assign_add(K.sum(y_true * y_pred))\n",
                "        self.actual_positives.assign_add(K.sum(y_true))\n",
                "\n",
                "    def result(self):\n",
                "        return self.true_positives / (self.actual_positives + K.epsilon())\n",
                "\n",
                "    def reset_states(self):\n",
                "        self.true_positives.assign(0)\n",
                "        self.actual_positives.assign(0)\n",
                "\n",
                "class F1Score(Metric):\n",
                "    def __init__(self, name='f1_score', **kwargs):\n",
                "        super(F1Score, self).__init__(name=name, **kwargs)\n",
                "        self.precision = Precision()\n",
                "        self.recall = Recall()\n",
                "\n",
                "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
                "        self.precision.update_state(y_true, y_pred)\n",
                "        self.recall.update_state(y_true, y_pred)\n",
                "\n",
                "    def result(self):\n",
                "        precision = self.precision.result()\n",
                "        recall = self.recall.result()\n",
                "        return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
                "\n",
                "    def reset_states(self):\n",
                "        self.precision.reset_states()\n",
                "        self.recall.reset_states()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f6db0cfc-5d4f-45f7-952c-ad45388155cd",
            "metadata": {},
            "outputs": [],
            "source": [
                "model = Sequential([\n",
                "    Conv2D(128, (3, 3), input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
                "    BatchNormalization(),\n",
                "    Activation('relu'),\n",
                "    MaxPooling2D((2, 2)),\n",
                "    Dropout(0.3),\n",
                "    \n",
                "    Conv2D(256, (3, 3)),\n",
                "    BatchNormalization(),\n",
                "    Activation('relu'),\n",
                "    MaxPooling2D((2, 2)),\n",
                "    Dropout(0.5),\n",
                "    \n",
                "    Conv2D(512, (3, 3)),\n",
                "    BatchNormalization(),\n",
                "    Activation('relu'),\n",
                "    MaxPooling2D((2, 2)),\n",
                "    Dropout(0.5),\n",
                "    \n",
                "    Flatten(),\n",
                "    Dense(512),\n",
                "    BatchNormalization(),\n",
                "    Activation('relu'),\n",
                "    Dropout(0.5),\n",
                "    \n",
                "    Dense(NUM_CLASSES, activation='softmax')\n",
                "])\n",
                "\n",
                "# Compile the model\n",
                "model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
                "              loss='categorical_crossentropy',\n",
                "              metrics=['accuracy', Precision(), Recall(), F1Score()])\n",
                "\n",
                "\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "43f29c95-ca64-403a-b3d6-70d9a7f2b37c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Definir os Callbacks\n",
                "\n",
                "# Para salvar o melhor modelo com base na acurácia de validação\n",
                "checkpoint = ModelCheckpoint(\"models/01_sem_data_augmentation.keras\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
                "\n",
                "# Parar o treinamento se não houver melhoria na loss após x epochs\n",
                "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
                "\n",
                "# Salvar para csv\n",
                "csv_logger = CSVLogger(f'logs/01_sem_data_augmentation_batch_size_{BATCH_SIZE}_image_size_{IMG_SIZE}.csv', append=True)\n",
                "\n",
                "# Reduzir a learning rate se não houver melhoria na loss após x epochs (lembrar de deixar este valor sempre menor que a patience no early_stopping!!)\n",
                "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a1bac321-549f-4e5f-9e2f-1da4b5aa721e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate steps per epoch\n",
                "steps_per_epoch = sum([gen.samples // BATCH_SIZE for gen in train_generators])\n",
                "\n",
                "# Train the model\n",
                "history = model.fit(\n",
                "    train_generator,\n",
                "    steps_per_epoch=steps_per_epoch,\n",
                "    epochs=NUM_EPOCHS,\n",
                "    validation_data=validation_generator,\n",
                "    validation_steps=validation_generator.samples // BATCH_SIZE,\n",
                "    callbacks=[checkpoint, early_stopping, csv_logger, reduce_lr]\n",
                ")\n",
                "\n",
                "\n",
                "# Evaluate the model\n",
                "results = model.evaluate(test_generator)\n",
                "loss, accuracy, precision, recall, f1_score = results[:5]\n",
                "print(f\"Test Loss: {loss}\")\n",
                "print(f\"Test Accuracy: {accuracy}\")\n",
                "print(f\"Test Precision: {precision}\")\n",
                "print(f\"Test Recall: {recall}\")\n",
                "print(f\"Test F1 Score: {f1_score}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7e0ce15d-3b9c-412d-b4fd-ecc041ac55fb",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training history\n",
                "plt.figure(figsize=(12, 8))\n",
                "plt.subplot(2, 1, 1)\n",
                "plt.plot(history.history['accuracy'], label='train_accuracy')\n",
                "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Accuracy')\n",
                "plt.ylim([0, 1])\n",
                "plt.legend(loc='lower right')\n",
                "plt.title('Training and Validation Accuracy')\n",
                "\n",
                "plt.subplot(2, 1, 2)\n",
                "plt.plot(history.history['val_precision'], label='val_precision')\n",
                "plt.plot(history.history['val_recall'], label='val_recall')\n",
                "plt.plot(history.history['val_f1_score'], label='val_f1_score')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Metrics')\n",
                "plt.ylim([0, 1])\n",
                "plt.legend(loc='lower right')\n",
                "plt.title('Validation Precision, Recall, F1 Score')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "plt.savefig(f'./plots/01_sem_data_augmentation.png')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}

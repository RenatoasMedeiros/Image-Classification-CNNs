{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "ddf0cd73",
            "metadata": {},
            "source": [
                "# Modelo sem Data Augmentation\n",
                "\n",
                "Nossa primeira abordagem foi a mais simples: utilizamos somente os dados do nosso dataset sem qualquer tipo de data augmentation, com o objetivo de analisar o comportamento do modelo assim definido.\n",
                "\n",
                "O principal desafio desta fase foi, sem dúvida, definir a estrutura base que utilizaríamos nos outros arquivos. Essa estrutura abrange a maneira como lemos os dados de treino, validação e teste dos diversos arquivos."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2776efbf-9015-4f0e-8bf0-111ce43e0108",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import tensorflow as tf\n",
                "import matplotlib.pyplot as plt\n",
                "import os\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D, BatchNormalization\n",
                "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, ReduceLROnPlateau\n",
                "from tensorflow.keras.optimizers import Adam\n",
                "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d5a0ea7b",
            "metadata": {},
            "source": [
                "\"os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\"\n",
                "\n",
                "Esta linha configura o TensorFlow para limitar a quantidade de mensagens de log que ele gera.\n",
                "O valor '2' faz com que apenas mensagens de erro e warnings sejam exibidas.\n",
                "\n",
                "\n",
                "Testamos também com multiplos BATCH_SIZES, mas 32 foi o que obteve melhores resultados (por mais incrível que pareça)\n",
                "\n",
                "Aqui estão os resultados com outros batch_sizes:\n",
                "\n",
                "BATCH_SIZE = 64, Resultado obtido: 0.8408453464508057\n",
                "BATCH_SIZE = 128, Resultado obtido: 0.8300280570983887\n",
                "BATCH_SIZE = 32, Resultado obtido: 0.8544671535491943\n",
                "\n",
                "(Importante realçar que estes valores estão a ser obtidos do ficheiro csv que geramos em cada execução, esses ficheiros estão a guardar os resultados de validação não de teste!)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2e706d27-0688-4988-9a6f-2666e784735c",
            "metadata": {},
            "outputs": [],
            "source": [
                "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
                "# CONSTANTES\n",
                "BATCH_SIZE = 32\n",
                "IMG_SIZE = 32\n",
                "NUM_CLASSES = 10  # nº classes para identificar\n",
                "NUM_EPOCHS = 60\n",
                "LEARNING_RATE = 0.001 # learning rate inicial"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a36f566b",
            "metadata": {},
            "source": [
                "# FOLDERS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ec2b77b4-9d5f-4ed6-8f1f-38ab7d9f91d5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define directories\n",
                "train_dirs = ['./dataset/train/train1', './dataset/train/train2', './dataset/train/train3', './dataset/train/train5']\n",
                "validation_dir = './dataset/validation'\n",
                "test_dir = './dataset/test'"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5d4efb11",
            "metadata": {},
            "source": [
                "# Normalização das imagens\n",
                "\n",
                "Aqui nós decidimos utilizar a biblioteca ImageDataGenerator para pré-processamento de imagens\n",
                "\n",
                "E com ela definimos as instâncias train_datagen, validation_datagen, e test_datagen são configuradas para apenas o rescale  dos valores dos pixels das imagens, normalizando-os para o intervalo [0, 1].\n",
                "\n",
                "\n",
                "Para cada diretório de treino listado em train_dirs, um gerador é criado, a função desses geradores é redimensionar o tamanho das imagens para o tamanho definido na constante acima, mas visto que são criados multiplos geradores para o treino precisamos de os combinar num só e para isso criamos a função *combined_generator* depois são criados de maneira similar os geradores de validation e test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1ce2b0fa-d958-482c-a087-d53f061c54bc",
            "metadata": {},
            "outputs": [],
            "source": [
                "# CRIAR OS GERADORES\n",
                "train_datagen = ImageDataGenerator(rescale=1./255)\n",
                "\n",
                "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
                "test_datagen = ImageDataGenerator(rescale=1./255)\n",
                "\n",
                "# training generators\n",
                "train_generators = [train_datagen.flow_from_directory(\n",
                "    train_dir,\n",
                "    target_size=(IMG_SIZE, IMG_SIZE),\n",
                "    batch_size=BATCH_SIZE,\n",
                "    class_mode='categorical') for train_dir in train_dirs]\n",
                "\n",
                "# Necessário para junstar os trainning generators\n",
                "def combined_generator(generators):\n",
                "    while True:\n",
                "        for generator in generators:\n",
                "            yield next(generator)\n",
                "\n",
                "train_generator = combined_generator(train_generators)\n",
                "\n",
                "# Validation e test generators\n",
                "validation_generator = validation_datagen.flow_from_directory(\n",
                "    validation_dir,\n",
                "    target_size=(IMG_SIZE, IMG_SIZE),\n",
                "    batch_size=BATCH_SIZE,\n",
                "    class_mode='categorical')\n",
                "\n",
                "test_generator = test_datagen.flow_from_directory(\n",
                "    test_dir,\n",
                "    target_size=(IMG_SIZE, IMG_SIZE),\n",
                "    batch_size=BATCH_SIZE,\n",
                "    class_mode='categorical')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "313698c7",
            "metadata": {},
            "source": [
                "# Definição das funções para mostrar as metricas de Precision, Recall e F1 Score"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "598663f6",
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras import backend as K\n",
                "from tensorflow.keras.metrics import Metric\n",
                "\n",
                "class Precision(Metric):\n",
                "    def __init__(self, name='precision', **kwargs):\n",
                "        super(Precision, self).__init__(name=name, **kwargs)\n",
                "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
                "        self.predicted_positives = self.add_weight(name='pp', initializer='zeros')\n",
                "\n",
                "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
                "        y_pred = K.round(y_pred)\n",
                "        y_true = K.cast(y_true, 'float32')\n",
                "        self.true_positives.assign_add(K.sum(y_true * y_pred))\n",
                "        self.predicted_positives.assign_add(K.sum(y_pred))\n",
                "\n",
                "    def result(self):\n",
                "        return self.true_positives / (self.predicted_positives + K.epsilon())\n",
                "\n",
                "    def reset_states(self):\n",
                "        self.true_positives.assign(0)\n",
                "        self.predicted_positives.assign(0)\n",
                "\n",
                "class Recall(Metric):\n",
                "    def __init__(self, name='recall', **kwargs):\n",
                "        super(Recall, self).__init__(name=name, **kwargs)\n",
                "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
                "        self.actual_positives = self.add_weight(name='ap', initializer='zeros')\n",
                "\n",
                "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
                "        y_pred = K.round(y_pred)\n",
                "        y_true = K.cast(y_true, 'float32')\n",
                "        self.true_positives.assign_add(K.sum(y_true * y_pred))\n",
                "        self.actual_positives.assign_add(K.sum(y_true))\n",
                "\n",
                "    def result(self):\n",
                "        return self.true_positives / (self.actual_positives + K.epsilon())\n",
                "\n",
                "    def reset_states(self):\n",
                "        self.true_positives.assign(0)\n",
                "        self.actual_positives.assign(0)\n",
                "\n",
                "class F1Score(Metric):\n",
                "    def __init__(self, name='f1_score', **kwargs):\n",
                "        super(F1Score, self).__init__(name=name, **kwargs)\n",
                "        self.precision = Precision()\n",
                "        self.recall = Recall()\n",
                "\n",
                "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
                "        self.precision.update_state(y_true, y_pred)\n",
                "        self.recall.update_state(y_true, y_pred)\n",
                "\n",
                "    def result(self):\n",
                "        precision = self.precision.result()\n",
                "        recall = self.recall.result()\n",
                "        return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
                "\n",
                "    def reset_states(self):\n",
                "        self.precision.reset_states()\n",
                "        self.recall.reset_states()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "667f1855",
            "metadata": {},
            "source": [
                "## Definição do Modelo\n",
                "\n",
                "Neste pedaço de código, definimos a arquitetura do nosso modelo com Sequential. O modelo possui diversas camadas:\n",
                "\n",
                "    Camadas Conv2D: Utilizadas para extrair características das imagens.\n",
                "\n",
                "    Camadas de Normalização de Batch: Normalizam os outputs das camadas convolucionais para acelerar o treinamento e melhorar a estabilidade.\n",
                "    \n",
                "    Camadas de Ativação com ReLU: Introduzem não-linearidades no modelo, permitindo a aprendizagem de funções complexas.\n",
                "    \n",
                "    Camadas MaxPooling: Reduzem a dimensionalidade das características extraídas, diminuindo a carga computacional e prevenindo o overfitting.\n",
                "    \n",
                "    Camadas de Dropout: Aplicamos dropout progressivamente maior até 50%, conforme recomendado, para prevenir o overfitting.\n",
                "    \n",
                "    Camada Dense e Camada de Saída: No final, adicionamos uma camada Dense seguida pela camada de saída com a função de ativação softmax, pois temos 10 classes possíveis."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e431284e",
            "metadata": {},
            "source": [
                "Escolhemos utilizar o otimizador Adam pois Combina vantagens do AdaGrad e RMSProp e em diversos forums era dito que o mesmo acelerava a convergência e lidava bem com grandes datasets (não que este pareça ser o caso mas também nao achamos problemas com datasets mais reduzidos)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f6db0cfc-5d4f-45f7-952c-ad45388155cd",
            "metadata": {},
            "outputs": [],
            "source": [
                "model = Sequential([\n",
                "    Conv2D(128, (3, 3), input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
                "    BatchNormalization(),\n",
                "    Activation('relu'),\n",
                "    MaxPooling2D((2, 2)),\n",
                "    Dropout(0.3),\n",
                "    \n",
                "    Conv2D(256, (3, 3)),\n",
                "    BatchNormalization(),\n",
                "    Activation('relu'),\n",
                "    MaxPooling2D((2, 2)),\n",
                "    Dropout(0.5),\n",
                "    \n",
                "    Conv2D(512, (3, 3)),\n",
                "    BatchNormalization(),\n",
                "    Activation('relu'),\n",
                "    MaxPooling2D((2, 2)),\n",
                "    Dropout(0.5),\n",
                "    \n",
                "    Flatten(),\n",
                "    Dense(512),\n",
                "    BatchNormalization(),\n",
                "    Activation('relu'),\n",
                "    Dropout(0.5),\n",
                "    \n",
                "    Dense(NUM_CLASSES, activation='softmax')\n",
                "])\n",
                "\n",
                "# Compile the model\n",
                "model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
                "              loss='categorical_crossentropy',\n",
                "              metrics=['accuracy', Precision(), Recall(), F1Score()])\n",
                "\n",
                "\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "aa4a0737",
            "metadata": {},
            "source": [
                "# Criar callbacks\n",
                "\n",
                "Callbacks que são utilizados durante o treino do modelo. \n",
                "\n",
                "Decidimos incluir o CSVLogger para registrar o progresso do treino do nosso modelo.\n",
                "\n",
                "Também optamos por utilizar o EarlyStopping, pois era provável que ocorresse overfitting após um certo número de epochs. Com ele, o treinamento é interrompido automaticamente se o modelo não apresentar melhorias significativas após um número específico de epochs, acelerando assim o processo de treino dos diferentes modelos.\n",
                "\n",
                "Em busca de mais otimização, encontramos o ReduceLROnPlateau, que permite reduzir a learning rate, conforme necessário (se nao melhorar de x em x epochs neste caso) para evitar oscilações ou estagnações no processo de treinamento, oque se comprovou eficiente!\n",
                "\n",
                "\n",
                "Resultado dos modelos sem EarlyStopping e ReduceLROnPlateau:\n",
                "\n",
                "Validation Accuracy:\n",
                "Validation Loss:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "43f29c95-ca64-403a-b3d6-70d9a7f2b37c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Definir os Callbacks\n",
                "\n",
                "# Para salvar o melhor modelo com base na acurácia de validação\n",
                "checkpoint = ModelCheckpoint(\"models/01_sem_data_augmentation_.keras\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
                "\n",
                "# Parar o treinamento se não houver melhoria na loss após x epochs\n",
                "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
                "\n",
                "# Salvar para csv\n",
                "csv_logger = CSVLogger(f'logs/01_sem_data_augmentation_batch_size_{BATCH_SIZE}_image_size_{IMG_SIZE}.csv', append=True)\n",
                "\n",
                "# Reduzir a learning rate se não houver melhoria na loss após x epochs (lembrar de deixar este valor sempre menor que a patience no early_stopping!!)\n",
                "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0b3cbb32",
            "metadata": {},
            "source": [
                "# Avaliação do modelo\n",
                "\n",
                "Aqui avaliamos o modelo treinado posteriormente e e chamamos os callbacks criados posteriormente."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a1bac321-549f-4e5f-9e2f-1da4b5aa721e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate steps per epoch\n",
                "steps_per_epoch = sum([gen.samples // BATCH_SIZE for gen in train_generators])\n",
                "\n",
                "# Train the model\n",
                "history = model.fit(\n",
                "    train_generator,\n",
                "    steps_per_epoch=steps_per_epoch,\n",
                "    epochs=NUM_EPOCHS,\n",
                "    validation_data=validation_generator,\n",
                "    validation_steps=validation_generator.samples // BATCH_SIZE,\n",
                "    callbacks=[checkpoint, early_stopping, csv_logger, reduce_lr]\n",
                ")\n",
                "\n",
                "# Evaluate the model\n",
                "# Evaluate the model\n",
                "results = model.evaluate(test_generator)\n",
                "loss, accuracy, precision, recall, f1_score = results[:5]\n",
                "print(f\"Test Loss: {loss}\")\n",
                "print(f\"Test Accuracy: {accuracy}\")\n",
                "print(f\"Test Precision: {precision}\")\n",
                "print(f\"Test Recall: {recall}\")\n",
                "print(f\"Test F1 Score: {f1_score}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7e0ce15d-3b9c-412d-b4fd-ecc041ac55fb",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training history\n",
                "plt.figure(figsize=(12, 8))\n",
                "plt.subplot(2, 1, 1)\n",
                "plt.plot(history.history['accuracy'], label='train_accuracy')\n",
                "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Accuracy')\n",
                "plt.ylim([0, 1])\n",
                "plt.legend(loc='lower right')\n",
                "plt.title('Training and Validation Accuracy')\n",
                "\n",
                "plt.subplot(2, 1, 2)\n",
                "plt.plot(history.history['val_precision'], label='val_precision')\n",
                "plt.plot(history.history['val_recall'], label='val_recall')\n",
                "plt.plot(history.history['val_f1_score'], label='val_f1_score')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Metrics')\n",
                "plt.ylim([0, 1])\n",
                "plt.legend(loc='lower right')\n",
                "plt.title('Validation Precision, Recall, F1 Score')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "plt.savefig(f'./plots/01_sem_data_augmentation.png')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
